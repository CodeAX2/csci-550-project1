{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1\n",
    "### Jacob Hofer, Flint Morgan, Joseph Winjum, Keith Filler\n",
    "---\n",
    "## Loading data\n",
    "Below is the code to load our data from the CSV into a Pandas DataFrame\n",
    "\n",
    "We are dropping the columns including semantic data, such as the product name, since we do not want to attempt to cluster based on those columns.\n",
    "\n",
    "We also drop the GFLOPS columns, as those are specific to GPUs, and we want to include both GPUs and CPUs.\n",
    "\n",
    "Lastly, we drop any rows that have missing data. \n",
    "\n",
    "We can't fill the empty values with the mean for the column because as the die size, transistors and frequency will increase quickly with year, and if we choose to put the mean it could miss represent the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process Size (nm)</th>\n",
       "      <th>TDP (W)</th>\n",
       "      <th>Die Size (mm^2)</th>\n",
       "      <th>Transistors (million)</th>\n",
       "      <th>Freq (MHz)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>2200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>3200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>1800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>3700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>2400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4844</th>\n",
       "      <td>40.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>334.0</td>\n",
       "      <td>2154.0</td>\n",
       "      <td>700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4845</th>\n",
       "      <td>40.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>416.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4846</th>\n",
       "      <td>28.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4849</th>\n",
       "      <td>40.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>1950.0</td>\n",
       "      <td>450.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4851</th>\n",
       "      <td>40.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>486.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3422 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Process Size (nm)  TDP (W)  Die Size (mm^2)  Transistors (million)  \\\n",
       "0                  65.0     45.0             77.0                  122.0   \n",
       "1                  14.0     35.0            192.0                 4800.0   \n",
       "3                  22.0     80.0            160.0                 1400.0   \n",
       "4                  45.0    125.0            258.0                  758.0   \n",
       "5                  22.0     95.0            160.0                 1400.0   \n",
       "...                 ...      ...              ...                    ...   \n",
       "4844               40.0    150.0            334.0                 2154.0   \n",
       "4845               40.0     20.0             80.0                   10.0   \n",
       "4846               28.0     21.0             68.0                  302.0   \n",
       "4849               40.0     75.0            332.0                 1950.0   \n",
       "4851               40.0     23.0            100.0                  486.0   \n",
       "\n",
       "      Freq (MHz)  \n",
       "0         2200.0  \n",
       "1         3200.0  \n",
       "3         1800.0  \n",
       "4         3700.0  \n",
       "5         2400.0  \n",
       "...          ...  \n",
       "4844       700.0  \n",
       "4845       416.0  \n",
       "4846       550.0  \n",
       "4849       450.0  \n",
       "4851       500.0  \n",
       "\n",
       "[3422 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"chip_dataset.csv\")\n",
    "\n",
    "Type = df['Type']\n",
    "# Drop semantic data\n",
    "df.pop(\"Unnamed: 0\");df.pop(\"Type\");df.pop(\"Foundry\");df.pop(\"Vendor\");df.pop(\"Product\");df.pop(\"Release Date\")\n",
    "# GFLOPS are specific to GPUs, so we exclude them here so we can also look at CPUs\n",
    "df.pop(\"FP16 GFLOPS\");df.pop(\"FP32 GFLOPS\");df.pop(\"FP64 GFLOPS\")\n",
    "df = df.dropna()\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "---\n",
    "### 2.1 What is the multivariate mean of the numerical data matrix (where categorical data have been converted to numerical values)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process Size (nm)          53.048510\n",
      "TDP (W)                    83.740795\n",
      "Die Size (mm^2)           200.003799\n",
      "Transistors (million)    2163.295441\n",
      "Freq (MHz)               1507.964641\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "multivariate_mean = np.mean(df, axis=0)\n",
    "print(multivariate_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 What is the covariance matrix of the numerical data matrix (where categorical data have been converted to numerical values)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'roundnp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroundnp\u001b[49m\u001b[38;5;241m.\u001b[39mcov(df\u001b[38;5;241m.\u001b[39mT)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\__init__.py:320\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tester\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Tester\n\u001b[1;32m--> 320\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;18m__name__\u001b[39m, attr))\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'roundnp'"
     ]
    }
   ],
   "source": [
    "np.roundnp.cov(df.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Choose 2 pairs of attributes that you think could be related. Create scatter plots of all 2 pairs and include these in your report, along with a description and analysis that summarizes why these pairs of attributes might be related, and how the scatter plots do or do not support this intuition.\n",
    "\n",
    "We believe that die size and transistors will be related because as the die size increases the total number of transistors could increase given the same size of transistor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_df = df.to_numpy()\n",
    "titles = df.columns\n",
    "for i in range(np.shape(np_df)[1]):\n",
    "    for j in range(np.shape(np_df)[1]):\n",
    "        if i != j and not i>j:\n",
    "            Title = titles[i]+\" vs \"+titles[j]\n",
    "            plt.figure()\n",
    "            plt.scatter(np_df[:,i],np_df[:,j])\n",
    "            plt.title(Title);plt.xlabel(titles[i]);plt.ylabel(titles[j])\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "#np_df[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Which range-normalized numerical attributes have the greatest sample covariance? What is their sample covariance? Create a scatter plot of these range-normalized attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import copy\n",
    "normalized_df = MinMaxScaler().fit_transform(np_df)\n",
    "normalized_df = normalized_df - np.mean(normalized_df,axis=0)\n",
    "cov_matrix = np.cov(normalized_df.T)\n",
    "cov_only = copy.copy(cov_matrix); np.fill_diagonal(cov_only,0)\n",
    "max_cov = cov_matrix.flat[np.abs(cov_only).argmax()]\n",
    "print(cov_matrix,\"\\n Max sample covariance:\",max_cov)\n",
    "x,y = np.where(abs(cov_matrix) == max_cov)[0]\n",
    "\n",
    "Title = titles[x]+\" vs \"+titles[y]\n",
    "plt.figure()\n",
    "plt.scatter(normalized_df[:,x],normalized_df[:,y])\n",
    "plt.title(Title);plt.xlabel(titles[x]);plt.ylabel(titles[y])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Which Z-score-normalized numerical attributes have the greatest correlation? What is their correlation? Create a scatter plot of these Z-score-normalized attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "z_df = (np_df-np.mean(np_df))/(np.std(np_df))\n",
    "corr = np.corrcoef(z_df.T)\n",
    "np.fill_diagonal(corr,0)\n",
    "max_corr = corr.flat[np.abs(corr).argmax()]\n",
    "print(corr,\"\\n Max sample correlation:\",max_corr)\n",
    "x,y = np.where(abs(corr) == max_corr)[0]\n",
    "\n",
    "Title = titles[x]+\" vs \"+titles[y]\n",
    "plt.figure()\n",
    "plt.scatter(z_df[:,x],z_df[:,y])\n",
    "plt.title(Title);plt.xlabel(titles[x]);plt.ylabel(titles[y])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 How many pairs of features have correlation greater than or equal to 0.5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(int(len(corr[corr>=0.5])/2),\"pairs of features have greater than or equal to 0.5 correlation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 How many pairs of features have negative sample covariance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(int(len(cov_only[cov_only<0])/2),\"pairs of features have negative sample covariance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 What is the total variance of the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The total variance of the data is\", np.trace(cov_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 What is the total variance of the data, restricted to the five features that have the 3 greatest sample variance?\n",
    "I took this to mean three features that have 3 greatest sample varience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variances = np.diagonal(cov_matrix)\n",
    "\n",
    "print(\"The total varience of the three features that have the greatest sample variance:\", sum(sorted(variances,reverse=True)[0:3]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering\n",
    "### Computing the Clusters\n",
    "The code below computes clusters in the data using our custom K-Means Clustering algorithm.\n",
    "\n",
    "We found that a good number of clusters is 3, as any higher K value tended to produce clusters containing a very low number of points.\n",
    "\n",
    "Our $\\epsilon$ value was also chosen to be 0.001, which dictates the threshold of centroid change that denotes the algorithm should terminate.\n",
    "\n",
    "The code that performs K-Means Clustering is available in `kmeans.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kmeans\n",
    "\n",
    "k = 3\n",
    "\n",
    "(centroids, assignments) = kmeans.kMeans(df, k, 0.001);\n",
    "for i, centroid in enumerate(centroids):\n",
    "    print(\"Cluster\", i, \"Centroid:\", centroid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Clusters\n",
    "Below is an example plot of our clustering. The colors of each point correspond to their assigned cluster.\n",
    "\n",
    "Cluster centroids are denoted by the larger black points.\n",
    "\n",
    "Changing the `xAxis` and `yAxis` values to any integer between 0 and 4 will change which two features are compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xAxis = 2\n",
    "yAxis = 4\n",
    "\n",
    "plt.scatter(df.values[:,xAxis], df.values[:,yAxis], c=assignments, s=5)\n",
    "plt.scatter(centroids[:,xAxis], centroids[:,yAxis], c='black', s=50)\n",
    "plt.xlabel(df.columns[xAxis])\n",
    "plt.ylabel(df.columns[yAxis]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBScan Based Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dbscan\n",
    "\n",
    "dbScanAlg = dbscan.DBScan(40, 0.6, -3)\n",
    "# dbscan works a bit better with normalized data\n",
    "normalizedDF = (df-df.mean())/df.std()\n",
    "(assignments, corePts, borderPts, noisePts) = dbScanAlg.runAlgorithm(normalizedDF);\n",
    "for i in assignments:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xAxis = 2\n",
    "yAxis = 4\n",
    "\n",
    "plt.scatter(df.values[:,xAxis], df.values[:,yAxis], c=assignments, s=5)\n",
    "plt.title(\"DBScan Clusters + Noise\")\n",
    "plt.xlabel(df.columns[xAxis])\n",
    "plt.ylabel(df.columns[yAxis]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in noisePts:\n",
    "    plt.plot(df.values[i,xAxis], df.values[i,yAxis], marker=\"o\", markerfacecolor='blue', markersize=3, markeredgecolor='blue')\n",
    "\n",
    "plt.title(\"DBScan Noise Points\")\n",
    "plt.xlabel(df.columns[xAxis])\n",
    "plt.ylabel(df.columns[yAxis]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 PCA graph in 2dim \n",
    "\n",
    "data is scaled b/c different units, two pca's are fit (one for 2Dim, one for no specified dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = StandardScaler(with_std=True,\n",
    "                        with_mean=True)\n",
    "df_scaled = scaler.fit_transform(df)\n",
    "\n",
    "pcadf = PCA() #unspecified number of components\n",
    "pcadf2 = PCA(n_components=2) #2 components\n",
    "pcadf.fit(df_scaled)\n",
    "pcadf2.fit(df_scaled)\n",
    "pcadf2.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pcadf2.transform(df_scaled)\n",
    "pcadf2.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph of 2D pca,\n",
    "\n",
    "\"Does it look like there are clusters\n",
    "in these two dimensions? If so, how many would you say there are?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, j = 0, 1 # which components\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "ax.scatter(scores[:,0], scores[:,1], s=5, color='green', alpha=0.3)\n",
    "ax.set_xlabel('PC%d' % (i+1))\n",
    "ax.set_ylabel('PC%d' % (j+1))\n",
    "for k in range(pcadf2.components_.shape[1]):\n",
    "    ax.arrow(0, 0, pcadf2.components_[i,k], pcadf2.components_[j,k],)\n",
    "    ax.text(pcadf2.components_[i,k],\n",
    "    pcadf2.components_[j,k],\n",
    "    df.columns[k],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same graph but scaling arrows and text for readability, \n",
    "graph flipped for perspective reasons but re-running will invert to normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_arrow = s_ = 3\n",
    "scores[:,1] *= -1\n",
    "pcadf.components_[1] *= -1 # flip the y-axis\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "ax.scatter(scores[:,0], scores[:,1], s=5)\n",
    "ax.set_xlabel('PC%d' % (i+1))\n",
    "ax.set_ylabel('PC%d' % (j+1))\n",
    "for k in range(pcadf2.components_.shape[1]):\n",
    "    ax.arrow(0, 0, s_*pcadf2.components_[i,k], s_*pcadf2.components_[\n",
    "        j,k])\n",
    "    ax.text(s_*pcadf2.components_[i,k],\n",
    "            s_*pcadf2.components_[j,k],\n",
    "            df.columns[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.std(0, ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explained variance and ratio for our two component pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcadf2.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcadf2.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 PCA unspecified components graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "graphs for 4.2\n",
    "\n",
    "\"Based on this plot, choose a number of principal components to reduce the dimensionality of the data. Report how many principal components will be used as well as the faction of total variance captured using this many components.\"\n",
    "\n",
    "Based on graphs we'd want 4, yadda yadda flesh out later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "ticks = np.arange(pcadf.n_components_)+1\n",
    "ax = axes[0]\n",
    "ax.plot(ticks,\n",
    "    pcadf.explained_variance_ratio_,\n",
    "    marker='o')\n",
    "ax.set_xlabel('Principal Component');\n",
    "ax.set_ylabel('Proportion of Variance Explained')\n",
    "ax.set_ylim([0,1])\n",
    "ax.set_xticks(ticks)\n",
    "ax = axes[1]\n",
    "ax.plot(ticks,\n",
    "    pcadf.explained_variance_ratio_.cumsum(),\n",
    "    marker='o')\n",
    "ax.set_xlabel('Principal Component')\n",
    "ax.set_ylabel('Cumulative Proportion of Variance Explained')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.set_xticks(ticks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcadf.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcadf.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3\n",
    "\n",
    "run a k-means analysis (use our own + sickit) for 4.3 and a DBSCAN analysis for \n",
    "4.4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
